{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbf9ba9-81c6-4218-a11c-41ff15651d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cd4fb8-126f-4c09-90dd-085c1f1b1c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Error fetching https://python.langchain.com/docs/integrations/chat/perplexity/ with attempt 1/3: Cannot connect to host python.langchain.com:443 ssl:default [Temporary failure in name resolution]. Retrying...\n",
      "Error fetching https://python.langchain.com/docs/integrations/chat/pipeshift/ with attempt 1/3: Cannot connect to host python.langchain.com:443 ssl:default [Temporary failure in name resolution]. Retrying...\n",
      "Error fetching https://python.langchain.com/docs/integrations/document_transformers/ai21_semantic_text_splitter/ with attempt 1/3: Cannot connect to host python.langchain.com:443 ssl:default [Temporary failure in name resolution]. Retrying...\n",
      "Error fetching https://python.langchain.com/docs/integrations/document_transformers/beautiful_soup/ with attempt 1/3: Cannot connect to host python.langchain.com:443 ssl:default [Temporary failure in name resolution]. Retrying...\n",
      "Error fetching https://python.langchain.com/docs/integrations/providers/runhouse/ with attempt 1/3: Cannot connect to host python.langchain.com:443 ssl:default [Temporary failure in name resolution]. Retrying...\n",
      "Fetching pages: 100%|############################################################| 1479/1479 [03:33<00:00,  6.93it/s]\n",
      "Fetching pages: 100%|##################################################################| 1/1 [00:00<00:00, 14.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://python.langchain.com/docs/integrations/document_loaders/sitemap/', 'loc': 'https://python.langchain.com/docs/integrations/document_loaders/sitemap/', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\nSitemap | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchProvidersAnthropicAWSGoogleHugging FaceMicrosoftOpenAIMoreProvidersAbsoAcreomActiveloop Deep LakeAerospikeAI21 LabsAimAINetworkAirbyteAirtableAlchemyAleph AlphaAlibaba CloudAnalyticDBAnnoyAnthropicAnyscaleApache Software FoundationApache DorisApifyAppleArangoDBArceeArcGISArgillaArizeArthurArxivAscendAskNewsAssemblyAIAstra DBAtlasAwaDBAWSAZLyricsBAAIBagelBagelDBBaichuanBaiduBananaBasetenBeamBeautiful SoupBibTeXBiliBiliBittensorBlackboardbookend.aiBoxBrave SearchBreebs (Open Knowledge)BrowserbaseBrowserlessByteDanceCassandraCerebrasCerebriumAIChaindeskChromaClarifaiClearMLClickHouseClickUpCloudflareClovaCnosDBCogniSwitchCohereCollege ConfidentialCometConfident AIConfluenceConneryContextCouchbaseCozeCrateDBC TransformersCTranslate2CubeDappierDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODataheraldDedocDeepInfraDeepSeekDeepSparseDiffbotDingoDBDiscordDiscord (community loader)DocArrayDoclingDoctranDocugamiDocusaurusDriaDropboxDSPyDuckDBDuckDuckGo SearchE2BEden AIElasticsearchElevenLabsEmbedchainEpsillaEtherscanEverly AIEverNoteExaFacebook - MetaFalkorDBFaunaFiddlerFigmaFireCrawlFireworks AIFlyteFMP Data (Financial Data Prep)Forefront AIFriendli AIGeopandasGitGitBookGitHubGitLabGoldenGoodfireGoogleSerper - Google Search APIGooseAIGPT4AllGradientGraph RAGGraphsignalGrobidGroqGutenbergHacker NewsHazy ResearchHeliconeHologresHTML to textHuaweiHugging FaceHyperbrowserIBMIEIT SystemsiFixitiFlytekIMSDbInfinispan VSInfinityInfinoIntelIuguJaguarJavelin AI GatewayJenkinsJina AIJohnsnowlabsJoplinKDB.AIKineticaKoboldAIKonkoKoNLPYKùzuLabel StudiolakeFSLanceDBLangChain Decorators ✨LangFair: Use-Case Level LLM Bias and Fairness AssessmentsLanternLindormLinkupLiteLLMLlamaIndexLlama.cppLlamaEdgellamafileLLMonitorLocalAILog10MariTalkMarqoMediaWikiDumpMeilisearchMemcachedMetalMicrosoftMilvusMindsDBMinimaxMistralAIMLflow AI Gateway for LLMsMLflowMLXModalModelScopeModern TreasuryMomentoMongoDBMongoDB AtlasMotherduckMotörheadMyScaleNAVERNeo4jNimbleNLPCloudNomicNotion DBNucliaNVIDIAObsidianOceanBaseOracle Cloud Infrastructure (OCI)OctoAIOllamaOntotext GraphDBOpenAIOpenLLMOpenSearchOpenWeatherMapOracleAI Vector SearchOutlineOutlinesPandasPaymanAIPebbloPerplexityPetalsPostgres EmbeddingPGVectorPineconePipelineAIPipeshiftPortkeyPredibasePrediction GuardPremAIPromptLayerPsychicPubMedPullMd LoaderPygmalionAIQdrantRAGatouillerank_bm25Ray ServeRebuffRedditRedisRemembrallReplicateRoamSema4 (fka Robocorp)RocksetRunhouseRWKV-4SalesforceSalute DevicesSambaNovaSAPScrapeGraph AISearchApiSearxNG Search APISemaDBSerpAPIShale ProtocolSingleStoreDBscikit-learnSlackSnowflakespaCySparkSparkLLMSpreedlySQLiteStack ExchangeStarRocksStochasticAIStreamlitStripeSupabase (Postgres)NebulaTairTelegramTencentTensorFlow DatasetsTiDBTigerGraphTigrisTiloresTogether AI2MarkdownTranswarpTrelloTrubricsTruLensTwitterTypesenseUnstructuredUpstageupstashUpTrainUSearchVDMSVearchVectaraVespavliteVoyageAIWeights & BiasesWeights & Biases tracingWeights & Biases trackingWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterxAIXataXorbits Inference (Xinference)YahooYandexYeager.aiYellowbrick01.AIYouYouTubeZepZhipu AIZillizComponentsChat modelsChat modelsAbsoAI21 LabsAlibaba Cloud PAI EASAnthropic[Deprecated] Experimental Anthropic Tools WrapperAnyscaleAzure OpenAIAzure ML EndpointBaichuan ChatBaidu QianfanAWS BedrockCerebrasCloudflare Workers AICohereCoze ChatDappier AIDatabricksDeepInfraDeepSeekEden AIErnie Bot ChatEverlyAIFireworksChatFriendliGigaChatGoodfireGoogle AIGoogle Cloud Vertex AIGPTRouterGroqChatHuggingFaceIBM watsonx.aiJinaChatKineticaKonkoLiteLLMLiteLLM RouterLlama 2 ChatLlama APILlamaEdgeLlama.cppmaritalkMiniMaxMistralAIMLXModelScopeMoonshotNaverNVIDIA AI EndpointsChatOCIModelDeploymentOCIGenAIChatOctoAIOllamaOpenAIOutlinesPerplexityPipeshiftChatPredictionGuardPremAIPromptLayer ChatOpenAIRekaSambaNovaCloudSambaStudioSnowflake CortexsolarSparkLLM ChatNebula (Symbl.ai)Tencent HunyuanTogetherTongyi QwenUpstagevLLM ChatVolc Enging MaasWriterxAIYandexGPTChatYIYuan2.0ZHIPU AIRetrieversRetrieversActiveloop Deep MemoryAmazon KendraArceeArxivAskNewsAzure AI SearchBedrock (Knowledge Bases)BM25BoxBREEBS (Open Knowledge)ChaindeskChatGPT pluginCohere rerankerCohere RAGDappierDocArrayDriaElasticSearch BM25ElasticsearchEmbedchainFlashRank rerankerFleet AI ContextGoogle DriveGoogle Vertex AI SearchGraph RAGIBM watsonx.aiJaguarDB Vector DatabaseKay.aiKinetica Vectorstore based RetrieverkNNLinkupSearchRetrieverLLMLingua Document CompressorLOTR (Merger Retriever)MetalMilvus Hybrid SearchNanoPQ (Product Quantization)needleNimbleOutlinePinecone Hybrid SearchPubMedQdrant Sparse VectorRAGatouilleRePhraseQueryRememberizerSEC filingSelf-querying retrieversSingleStoreDBSVMTavilySearchAPITF-IDF**NeuralDB**VespaWikipediaYou.comZep CloudZep Open SourceZilliz Cloud PipelineTools/ToolkitsToolsAINetwork ToolkitAlpha VantageAmadeus ToolkitApify ActorArXivAskNewsAWS LambdaAzure AI Services ToolkitAzure Cognitive Services ToolkitAzure Container Apps dynamic sessionsShell (bash)Bearly Code InterpreterBing SearchBrave SearchCassandra Database ToolkitCDPChatGPT PluginsClickUp ToolkitCogniswitch ToolkitConnery Toolkit and ToolsDall-E Image GeneratorDappierDatabricks Unity Catalog (UC)DataForSEODataheraldDuckDuckGo SearchDiscordE2B Data AnalysisEden AIElevenLabs Text2SpeechExa SearchFile SystemFinancialDatasets ToolkitFMP DataGithub ToolkitGitlab ToolkitGmail ToolkitGolden QueryGoogle BooksGoogle Cloud Text-to-SpeechGoogle DriveGoogle FinanceGoogle ImagenGoogle JobsGoogle LensGoogle PlacesGoogle ScholarGoogle SearchGoogle SerperGoogle TrendsGradioGraphQLHuggingFace Hub ToolsHuman as a toolIFTTT WebHooksInfobipIonic Shopping ToolJenkinsJina SearchJira ToolkitJSON ToolkitLemon AgentLinkupSearchToolMemorizeMojeek SearchMultiOn ToolkitNASA ToolkitNuclia UnderstandingNVIDIA Riva: ASR and TTSOffice365 ToolkitOpenAPI ToolkitNatural Language API ToolkitsOpenWeatherMapOracle AI Vector Search: Generate SummaryPandas DataframePassio NutritionAIPaymanAIPlayWright Browser ToolkitPolygon IO Toolkit and ToolsPowerBI ToolkitPubMedPython REPLReddit SearchRequests ToolkitRiza Code InterpreterRobocorp ToolkitSalesforceSceneXplainScrapeGraphSearchApiSearxNG SearchSemantic Scholar API ToolSerpAPISlack ToolkitSpark SQL ToolkitSQLDatabase ToolkitStackExchangeSteam ToolkitStripeTavily SearchTiloresTwilioUpstageWikidataWikipediaWolfram AlphaYahoo Finance NewsYou.com SearchYouTubeZapier Natural Language ActionsZenGuard AIDocument loadersDocument loadersacreomAirbyteLoaderAirbyte CDK (Deprecated)Airbyte Gong (Deprecated)Airbyte Hubspot (Deprecated)Airbyte JSON (Deprecated)Airbyte Salesforce (Deprecated)Airbyte Shopify (Deprecated)Airbyte Stripe (Deprecated)Airbyte Typeform (Deprecated)Airbyte Zendesk Support (Deprecated)AirtableAlibaba Cloud MaxComputeAmazon TextractApify DatasetArcGISArxivLoaderAssemblyAI Audio TranscriptsAstraDBAsync ChromiumAsyncHtmlAthenaAWS S3 DirectoryAWS S3 FileAZLyricsAzure AI DataAzure Blob Storage ContainerAzure Blob Storage FileAzure AI Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBoxBrave SearchBrowserbaseBrowserlessBSHTMLLoaderCassandraChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCouchbaseCSVCube Semantic LayerDatadog LogsDedocDiffbotDiscordDoclingDocugamiDocusaurusDropboxDuckDBEmailEPubEtherscanEverNoteexample_dataFacebook ChatFaunaFigmaFireCrawlGeopandasGitGitBookGitHubGlue CatalogGoogle AlloyDB for PostgreSQLGoogle BigQueryGoogle BigtableGoogle Cloud SQL for SQL serverGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle Firestore in Datastore ModeGoogle DriveGoogle El Carro for Oracle WorkloadsGoogle Firestore (Native Mode)Google Memorystore for RedisGoogle SpannerGoogle Speech-to-Text Audio TranscriptsGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetHyperbrowserLoaderiFixitImagesImage captionsIMSDbIuguJoplinJSONLoaderJupyter NotebookKineticalakeFSLangSmithLarkSuite (FeiShu)LLM SherpaMastodonMathPixPDFLoaderMediaWiki DumpMerge Documents LoadermhtmlMicrosoft ExcelMicrosoft OneDriveMicrosoft OneNoteMicrosoft PowerPointMicrosoft SharePointMicrosoft WordNear BlockchainModern TreasuryMongoDBNeedle Document LoaderNews URLNotion DB 2/2NucliaObsidianOpen Document Format (ODT)Open City DataOracle Autonomous DatabaseOracle AI Vector Search: Document ProcessingOrg-modePandas DataFrameparsersPDFMinerLoaderPDFPlumberPebblo Safe DocumentLoaderPolars DataFramePsychicPubMedPullMdLoaderPyMuPDFLoaderPyPDFDirectoryLoaderPyPDFium2LoaderPyPDFLoaderPySparkQuipReadTheDocs DocumentationRecursive URLRedditRoamRocksetrspaceRSS FeedsRSTscrapflyScrapingAntSitemapSlackSnowflakeSource CodeSpiderSpreedlyStripeSubtitleSurrealDBTelegramTencent COS DirectoryTencent COS FileTensorFlow DatasetsTiDB2MarkdownTOMLTrelloTSVTwitterUnstructuredUnstructuredMarkdownLoaderUnstructuredPDFLoaderUpstageURLVsdxWeatherWebBaseLoaderWhatsApp ChatWikipediaUnstructuredXMLLoaderXorbits Pandas DataFrameYouTube audioYouTube transcriptsYoutubeLoaderDLYuqueZeroxPDFLoaderVector storesVector storesActiveloop Deep LakeAerospikeAlibaba Cloud OpenSearchAnalyticDBAnnoyApache DorisApertureDBAstra DB Vector StoreAtlasAwaDBAzure Cosmos DB Mongo vCoreAzure Cosmos DB No SQLAzure AI SearchBagelBagelDBBaidu Cloud ElasticSearch VectorSearchBaidu VectorDBApache CassandraChromaClarifaiClickHouseCouchbaseDashVectorDatabricksDingoDBDocArray HnswSearchDocArray InMemorySearchAmazon Document DBDuckDBChina Mobile ECloud ElasticSearch VectorSearchElasticsearchEpsillaFaissFaiss (Async)FalkorDBVectorStoreGoogle AlloyDB for PostgreSQLGoogle BigQuery Vector SearchGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLFirestoreGoogle Memorystore for RedisGoogle SpannerGoogle Vertex AI Feature StoreGoogle Vertex AI Vector SearchHippoHologresInfinispanJaguar Vector DatabaseKDB.AIKineticaLanceDBLanternLindormLLMRailsManticoreSearch VectorStoreMarqoMeilisearchAmazon MemoryDBMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOceanbaseOpenSearchOracle AI Vector Search: Vector StorePathwayPostgres EmbeddingPGVecto.rsPGVectorPineconeQdrantRedisRelytRocksetSAP HANA Cloud Vector EngineScaNNSemaDBSingleStoreDBscikit-learnSQLiteVecSQLite-VSSSQLServerStarRocksSupabase (Postgres)SurrealDBTablestoreTairTencent Cloud VectorDBThirdAI NeuralDBTiDB VectorTigrisTileDBTimescale Vector (Postgres)TypesenseUpstash VectorUSearchValdIntel\\'s Visual Data Management System (VDMS)VearchVectaraVespaviking DBvliteWeaviateXataYellowbrickZepZep CloudZillizEmbedding modelsEmbedding modelsAI21Aleph AlphaAnyscaleascendAwaDBAzureOpenAIBaichuan Text EmbeddingsBaidu QianfanBedrockBGE on Hugging FaceBookend AIClarifaiCloudflare Workers AIClova EmbeddingsCohereDashScopeDatabricksDeepInfraEDEN AIElasticsearchEmbaasERNIEFake EmbeddingsFastEmbed by QdrantFireworksGigaChatGoogle Generative AI EmbeddingsGoogle Vertex AIGPT4AllGradientHugging FaceIBM watsonx.aiInfinityInstruct Embeddings on Hugging FaceIPEX-LLM: Local BGE Embeddings on Intel CPUIPEX-LLM: Local BGE Embeddings on Intel GPUIntel® Extension for Transformers Quantized Text EmbeddingsJinaJohn Snow LabsLASER Language-Agnostic SEntence Representations Embeddings by Meta AILindormLlama.cppllamafileLLMRailsLocalAIMiniMaxMistralAImodel2vecModelScopeMosaicMLNaverNLP CloudNomicNVIDIA NIMsOracle Cloud Infrastructure Generative AIOllamaOpenClipOpenAIOpenVINOEmbedding Documents using Optimized and Quantized EmbeddersOracle AI Vector Search: Generate EmbeddingsOVHcloudPinecone EmbeddingsPredictionGuardEmbeddingsPremAISageMakerSambaStudioSelf HostedSentence Transformers on Hugging FaceSolarSpaCySparkLLM Text EmbeddingsTensorFlow HubText Embeddings InferenceTextEmbed - Embedding Inference ServerTitan TakeoffTogether AIUpstageVolc EngineVoyage AIXorbits inference (Xinference)YandexGPTZhipuAIOtherComponentsDocument loadersSitemapOn this pageSitemap\\nExtends from the WebBaseLoader, SitemapLoader loads a sitemap from a given URL, and then scrapes and loads all pages in the sitemap, returning each page as a Document.\\nThe scraping is done concurrently. There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren\\'t concerned about being a good citizen, or you control the scrapped server, or don\\'t care about load you can increase this limit. Note, while this will speed up the scraping process, it may cause the server to block you. Be careful!\\nOverview\\u200b\\nIntegration details\\u200b\\nClassPackageLocalSerializableJS supportSiteMapLoaderlangchain_community\\x00\\x00✅❌✅\\nLoader features\\u200b\\nSourceDocument Lazy LoadingNative Async SupportSiteMapLoader✅❌\\nSetup\\u200b\\nTo access SiteMap document loader you\\'ll need to install the langchain-community integration package.\\nCredentials\\u200b\\nNo credentials are needed to run this.\\nIf you want to get automated best in-class tracing of your model calls you can also set your LangSmith API key by uncommenting below:\\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nInstallation\\u200b\\nInstall langchain_community.\\n%pip install -qU langchain-community\\nFix notebook asyncio bug\\u200b\\nimport nest_asyncionest_asyncio.apply()\\nInitialization\\u200b\\nNow we can instantiate our model object and load documents:\\nfrom langchain_community.document_loaders.sitemap import SitemapLoaderAPI Reference:SitemapLoader\\nsitemap_loader = SitemapLoader(web_path=\"https://api.python.langchain.com/sitemap.xml\")\\nLoad\\u200b\\ndocs = sitemap_loader.load()docs[0]\\nFetching pages: 100%|##########| 28/28 [00:04<00:00,  6.83it/s]\\nDocument(metadata={\\'source\\': \\'https://api.python.langchain.com/en/stable/\\', \\'loc\\': \\'https://api.python.langchain.com/en/stable/\\', \\'lastmod\\': \\'2024-05-15T00:29:42.163001+00:00\\', \\'changefreq\\': \\'weekly\\', \\'priority\\': \\'1\\'}, page_content=\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Python API Reference Documentation.\\\\n\\\\n\\\\nYou will be automatically redirected to the new location of this page.\\\\n\\\\n\\')\\nprint(docs[0].metadata)\\n{\\'source\\': \\'https://api.python.langchain.com/en/stable/\\', \\'loc\\': \\'https://api.python.langchain.com/en/stable/\\', \\'lastmod\\': \\'2024-05-15T00:29:42.163001+00:00\\', \\'changefreq\\': \\'weekly\\', \\'priority\\': \\'1\\'}\\nYou can change the requests_per_second parameter to increase the max concurrent requests. and use requests_kwargs to pass kwargs when send requests.\\nsitemap_loader.requests_per_second = 2# Optional: avoid `[SSL: CERTIFICATE_VERIFY_FAILED]` issuesitemap_loader.requests_kwargs = {\"verify\": False}\\nLazy Load\\u200b\\nYou can also load the pages lazily in order to minimize the memory load.\\npage = []for doc in sitemap_loader.lazy_load():    page.append(doc)    if len(page) >= 10:        # do some paged operation, e.g.        # index.upsert(page)        page = []\\nFetching pages: 100%|##########| 28/28 [00:01<00:00, 19.06it/s]\\nFiltering sitemap URLs\\u200b\\nSitemaps can be massive files, with thousands of URLs.  Often you don\\'t need every single one of them.  You can filter the URLs by passing a list of strings or regex patterns to the filter_urls parameter.  Only URLs that match one of the patterns will be loaded.\\nloader = SitemapLoader(    web_path=\"https://api.python.langchain.com/sitemap.xml\",    filter_urls=[\"https://api.python.langchain.com/en/latest\"],)documents = loader.load()\\ndocuments[0]\\nDocument(page_content=\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Python API Reference Documentation.\\\\n\\\\n\\\\nYou will be automatically redirected to the new location of this page.\\\\n\\\\n\\', metadata={\\'source\\': \\'https://api.python.langchain.com/en/latest/\\', \\'loc\\': \\'https://api.python.langchain.com/en/latest/\\', \\'lastmod\\': \\'2024-02-12T05:26:10.971077+00:00\\', \\'changefreq\\': \\'daily\\', \\'priority\\': \\'0.9\\'})\\nAdd custom scraping rules\\u200b\\nThe SitemapLoader uses beautifulsoup4 for the scraping process, and it scrapes every element on the page by default. The SitemapLoader constructor accepts a custom scraping function. This feature can be helpful to tailor the scraping process to your specific needs; for example, you might want to avoid scraping headers or navigation elements.\\nThe following example shows how to develop and use a custom function to avoid navigation and header elements.\\nImport the beautifulsoup4 library and define the custom function.\\npip install beautifulsoup4\\nfrom bs4 import BeautifulSoupdef remove_nav_and_header_elements(content: BeautifulSoup) -> str:    # Find all \\'nav\\' and \\'header\\' elements in the BeautifulSoup object    nav_elements = content.find_all(\"nav\")    header_elements = content.find_all(\"header\")    # Remove each \\'nav\\' and \\'header\\' element from the BeautifulSoup object    for element in nav_elements + header_elements:        element.decompose()    return str(content.get_text())\\nAdd your custom function to the SitemapLoader object.\\nloader = SitemapLoader(    \"https://api.python.langchain.com/sitemap.xml\",    filter_urls=[\"https://api.python.langchain.com/en/latest/\"],    parsing_function=remove_nav_and_header_elements,)\\nLocal Sitemap\\u200b\\nThe sitemap loader can also be used to load local files.\\nsitemap_loader = SitemapLoader(web_path=\"example_data/sitemap.xml\", is_local=True)docs = sitemap_loader.load()\\nAPI reference\\u200b\\nFor detailed documentation of all SiteMapLoader features and configurations head to the API reference: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sitemap.SitemapLoader.html#langchain_community.document_loaders.sitemap.SitemapLoader\\nRelated\\u200b\\n\\nDocument loader conceptual guide\\nDocument loader how-to guides\\nEdit this pageWas this page helpful?PreviousScrapingAntNextSlackOverviewIntegration detailsLoader featuresSetupCredentialsInstallationFix notebook asyncio bugInitializationLoadLazy LoadFiltering sitemap URLsAdd custom scraping rulesLocal SitemapAPI referenceRelatedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DocusaurusLoader\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "loader = DocusaurusLoader(\"https://python.langchain.com\")\n",
    "docs = loader.load()\n",
    "loader = DocusaurusLoader(\n",
    "    \"https://python.langchain.com\",\n",
    "    filter_urls=[\n",
    "        \"https://python.langchain.com/docs/integrations/document_loaders/sitemap\"\n",
    "    ],\n",
    ")\n",
    "documents = loader.load()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d13d1a3-0e8a-446c-aea1-48a22d40c5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocalFileStore\n\u001b[1;32m      5\u001b[0m store \u001b[38;5;241m=\u001b[39m LocalFileStore(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cache/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m underlying_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-3-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Avoiding unnecessary costs by caching the embeddings.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m EMBEDDINGS \u001b[38;5;241m=\u001b[39m CacheBackedEmbeddings\u001b[38;5;241m.\u001b[39mfrom_bytes_store(\n\u001b[1;32m     12\u001b[0m     underlying_embeddings, store, namespace\u001b[38;5;241m=\u001b[39munderlying_embeddings\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m     13\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:338\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    337\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client:\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "underlying_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "# Avoiding unnecessary costs by caching the embeddings.\n",
    "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014eb47-e99d-4fb2-9c81-474bf7a9f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3865a-500c-4c7d-b7a2-dcfcfaae6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "llm = VertexAI(model_name=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7000aba-e64d-406d-880a-c3dc29fec3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45d2b7-b566-45c6-b356-33385155ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49471e2-b7e7-42bf-ab03-d31de8a72271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
