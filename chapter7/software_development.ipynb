{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "text = \"\"\"\n",
    "def calculate_primes(n):\n",
    "    \\\"\\\"\\\"Create a list of consecutive integers from 2 up to N.\n",
    "\n",
    "    For example:\n",
    "    >>> calculate_primes(20)\n",
    "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
    "    \\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0891558b341919ba5924cdd744c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a197453ecdcd406ca5c7ce9a799ba2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85255e1b102b4a9bbf0bbdd4e97afb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84336689538d4c7c861f0c7a35198a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76de92b94a544482894c38a4fa418daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87779a245974c47a1b4f4b56af63cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587c2c31d2604d638afb46e48d874c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725247dd91a3435db4333bafe87bf63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675611fd07e040feaf508beba963707b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f5c3d6ca6640ceb83ee6c7840c2c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ea6ed3e94547b6a1454d89e680a86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224951/3744229729.py:20: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/tmp/ipykernel_224951/3744229729.py:34: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = llm(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def calculate_primes(n):\n",
      "    \"\"\"Create a list of consecutive integers from 2 up to N.\n",
      "\n",
      "    For example:\n",
      "    >>> calculate_primes(20)\n",
      "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for i in range(2, n + 1):\n",
      "        if is_prime(i):\n",
      "            primes.append(i)\n",
      "    return primes\n",
      "\n",
      "def is_prime(n):\n",
      "    \"\"\"Return True if n is prime.\"\"\"\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def main():\n",
      "    \"\"\"Get user input and print the list of primes.\"\"\"\n",
      "    n = int(input(\"Enter a number: \"))\n",
      "    primes = calculate_primes(n)\n",
      "    print(primes)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "<|file_separator|>\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Choose a more up-to-date model\n",
    "checkpoint = \"google/codegemma-2b\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "# Integrate the pipeline with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Define the input text\n",
    "text = \"\"\"\n",
    "def calculate_primes(n):\n",
    "    \\\"\\\"\\\"Create a list of consecutive integers from 2 up to N.\n",
    "\n",
    "    For example:\n",
    "    >>> calculate_primes(20)\n",
    "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
    "    \\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# Use the LangChain LLM to generate text\n",
    "output = llm(text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that downloading and loading of the models can take a few minutes. If you're getting an error saying you \"cannot access a gated repo\" when trying to use a URL with Langchain, it means you're attempting to access a private repository on Hugging Face that requires authentication with a personal access token to view or use the model; you need to create a Hugging Face access token and set it as an environment variable named \"HF_TOKEN\" to access the gated repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def calculate_primes(n):\n",
      "    \"\"\"Create a list of consecutive integers from 2 up to N.\n",
      "\n",
      "    For example:\n",
      "    >>> calculate_primes(20)\n",
      "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for i in range(2, n + 1):\n",
      "        if is_prime(i):\n",
      "            primes.append(i)\n",
      "    return primes\n",
      "\n",
      "def is_prime(n):\n",
      "    \"\"\"Return True if n is prime.\"\"\"\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def main():\n",
      "    \"\"\"Get user input and print the list of primes.\"\"\"\n",
      "    n = int(input(\"Enter a number: \"))\n",
      "    primes = calculate_primes(n)\n",
      "    print(primes)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "<|file_separator|>\n"
     ]
    }
   ],
   "source": [
    "completion = pipe(text)\n",
    "print(completion[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def calculate_primes(n):\n",
      "    \"\"\"Create a list of consecutive integers from 2 up to N.\n",
      "\n",
      "    For example:\n",
      "    >>> calculate_primes(20)\n",
      "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for i in range(2, n + 1):\n",
      "        if is_prime(i):\n",
      "            primes.append(i)\n",
      "    return primes\n",
      "\n",
      "def is_prime(n):\n",
      "    \"\"\"Return True if n is prime.\"\"\"\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def main():\n",
      "    \"\"\"Get user input and print the list of primes.\"\"\"\n",
      "    n = int(input(\"Enter a number: \"))\n",
      "    primes = calculate_primes(n)\n",
      "    print(primes)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "<|file_separator|>\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def calculate_primes(n):\n",
      "    \"\"\"Create a list of consecutive integers from 2 up to N.\n",
      "\n",
      "    For example:\n",
      "    >>> calculate_primes(20)\n",
      "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for i in range(2, n+1):\n",
      "        if all(i % j != 0 for j in range(2, int(i**0.5) + 1)):\n",
      "            primes.append(i)\n",
      "    return primes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Choose a lightweight model good for code generation\n",
    "repo_id = \"bigcode/starcoder\"\n",
    "\n",
    "# Initialize the HuggingFaceHub LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_length\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use the LangChain LLM to generate text\n",
    "output = llm.invoke(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To calculate the prime numbers up to a given number N, we can follow the Sieve of Eratosthenes algorithm. Here\\'s a step-by-step approach to solve the problem:\\n\\n```python\\ndef calculate_primes(n):\\n    \"\"\"Create a list of consecutive integers from 2 up to N.\"\"\"\\n    # Create a list of consecutive integers from 2 to N\\n    numbers = list(range(2, n + 1))\\n    \\n    # Initialize an empty list to store the prime numbers\\n    primes = []\\n    \\n    while numbers:\\n        # Take the first number as prime\\n        prime = numbers[0]\\n        primes.append(prime)\\n        \\n        # Remove all multiples of the prime number from the list\\n        numbers = [num for num in numbers if num % prime != 0]\\n    \\n    return primes\\n```\\n\\nStep-by-step explanation:\\n\\n1. We create a list called `numbers` that contains consecutive integers from 2 to N using the `range()` function.\\n\\n2. We initialize an empty list called `primes` to store the prime numbers that we will calculate.\\n\\n3. We start a loop that continues as long as there are numbers in the `numbers` list.\\n\\n4. Inside the loop, we take the first number from the `numbers` list and consider it as a prime number. We append this prime number to the `primes` list.\\n\\n5. We remove all the multiples of the prime number from the `numbers` list using a list comprehension. The list comprehension creates a new list containing only the numbers that are not divisible by the current prime number.\\n\\n6. We repeat steps 4 and 5 until the `numbers` list becomes empty. This ensures that we have considered all the numbers up to N and identified the prime numbers among them.\\n\\n7. Finally, we return the `primes` list, which contains all the prime numbers up to N.\\n\\nLet\\'s test the function with the given example:\\n\\n```python\\nprint(calculate_primes(20))\\n```\\n\\nOutput:\\n```\\n[2, 3, 5, 7, 11, 13, 17, 19]\\n```\\n\\nThe function correctly returns the list of prime numbers up to 20.\\n\\nThis implementation follows the Sieve of Eratosthenes algorithm, which efficiently calculates prime numbers by repeatedly marking the multiples of each prime number and removing them from the list of potential primes.' additional_kwargs={} response_metadata={'id': 'msg_01AgT8Amfrha3g7knHDJDgEp', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 93, 'output_tokens': 552}} id='run-5e02b082-f4c0-4273-a076-63b381264f9d-0' usage_metadata={'input_tokens': 93, 'output_tokens': 552, 'total_tokens': 645, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Let's think step by step.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "print(llm_chain.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224951/3242248507.py:8: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "/tmp/ipykernel_224951/3242248507.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent(\"What is 2 + 2?\")\n",
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is 2 + 2?', 'output': '4'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.language_models.fake import FakeListLLM\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "responses = [\"Action: Python_REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "tools = [PythonREPLTool()]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "result = agent(\"What is 2 + 2?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can write a Python script to find the prime numbers up to 20.\n",
      "Action: Python_REPL\n",
      "Action Input: def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "primes = [num for num in range(2, 21) if is_prime(num)]\n",
      "print(primes)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: [2, 3, 5, 7, 11, 13, 17, 19]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What are the prime numbers until 20?', 'output': '[2, 3, 5, 7, 11, 13, 17, 19]'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "result = agent(\"What are the prime numbers until 20?\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
