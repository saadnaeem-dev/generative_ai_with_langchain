{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c04be25-aa04-44a3-9a84-5ed7513e9f30",
   "metadata": {},
   "source": [
    "**Make sure you load the API keys for cloud providers!**\n",
    "\n",
    "You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d587a3c9-4cc8-4b1d-9b64-93f323a9b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables, the keys\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "# for the keys - as explained early in chapter 2\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a472b9-332a-44aa-93a8-e45cb3b0cedc",
   "metadata": {},
   "source": [
    "# Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16334895-b204-41e8-8403-e801b0fea004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How does climate change impact the environment?\n",
      "2. What are the consequences of climate change on our planet?\n",
      "3. Can you explain the various effects of climate change on Earth?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "expansion_template = \"\"\"Given the user question: {question}\n",
    "Generate three alternative versions that express the same information need but with different wording:\n",
    "1.\"\"\"\n",
    "\n",
    "expansion_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=expansion_template\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate expanded queries\n",
    "original_query = \"What are the effects of climate change?\"\n",
    "expanded_queries = expansion_chain.invoke(original_query)\n",
    "print(expanded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a49f0-fcd6-4865-aa69-2f0b7977fce2",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embeddings (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad253e09-2260-412c-bc4d-22f784d84f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True\n",
    ")\n",
    "documents = loader.load()\n",
    "embedder = OpenAIEmbeddings()\n",
    "embeddings = embedder.embed_documents([doc.page_content for doc in documents])\n",
    "vector_db = FAISS.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf9ebb7-899b-4a9c-b0b5-0e42ee277afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Create prompt for generating hypothetical document\n",
    "hyde_template = \"\"\"Based on the question: {question}\n",
    "Write a passage that could contain the answer to this question:\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=hyde_template\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate hypothetical document\n",
    "query = \"What dietary changes can reduce carbon footprint?\"\n",
    "hypothetical_doc = hyde_chain.invoke(query)\n",
    "\n",
    "# Use the hypothetical document for retrieval\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(hypothetical_doc)\n",
    "results = vector_db.similarity_search_by_vector(embedded_query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3693a8-de8e-44b6-86e1-7397e8054f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='439aaf5a-e7e9-496e-b3fe-004bf5604332', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(id='ef97752a-01e2-4819-ad07-c82397341164', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(id='9429579b-db33-43e8-88a7-f9831aaaaa1d', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d7d39-cc5a-4a07-b67b-aac2206ae79e",
   "metadata": {},
   "source": [
    "# Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90349445-a56d-48e8-a02b-930e6d7c22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Create a basic retriever from the vector store\n",
    "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "compressed_docs = compression_retriever.invoke(\"How do transformers work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dbdea69-104a-412f-b8ad-b2f7ff96ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018.')]\n"
     ]
    }
   ],
   "source": [
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bc16d-96f0-415d-bb5e-3cefd0638d5f",
   "metadata": {},
   "source": [
    "# Maximum Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1d5b65-bd5e-41f4-a523-4b9634f55b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='ef97752a-01e2-4819-ad07-c82397341164', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(id='3334753a-5cfa-4a22-963b-b6b0c4baf8d0', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'), Document(id='9429579b-db33-43e8-88a7-f9831aaaaa1d', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'), Document(id='439aaf5a-e7e9-496e-b3fe-004bf5604332', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(id='365f22f9-6a44-4eb9-b00c-5a0ababeb568', metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "mmr_results = vector_db.max_marginal_relevance_search(\n",
    "    query=\"What are transformer models?\",\n",
    "    k=5, # Number of documents to return\n",
    "    fetch_k=20, # Number of documents to initially fetch\n",
    "    lambda_mult=0.5 # Diversity parameter (0 = max diversity, 1 = max relevance)\n",
    ")\n",
    "print(mmr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216aa562-d716-4cc1-a713-eaf29a0912b5",
   "metadata": {},
   "source": [
    "# Source Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac233fb-0d93-4874-b372-098e7859afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer models are a type of neural network architecture that relies on self-attention mechanisms to process input data. They were first introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017 [1]. One example of a transformer model is BERT, which utilizes bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks [2]. Another example is the GPT series of models, which are autoregressive transformers that predict the next token based on previous tokens [3].\n",
      "\n",
      "Reference List:\n",
      "[1] Neural Network Review 2021, page 42\n",
      "[2] Introduction to NLP, page 137\n",
      "[3] Large Language Models Survey, page 89\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n",
    "        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n",
    "        metadata={\"source\": \"Introduction to NLP\", \"page\": 137}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n",
    "        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a vector store and retriever\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Source attribution prompt template\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a precise AI assistant that provides well-sourced information.\n",
    "Answer the following question based ONLY on the provided sources. For each fact or claim in your answer,\n",
    "include a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "Your answer:\n",
    "\"\"\")\n",
    "\n",
    "# Create a source-formatted string from documents\n",
    "def format_sources_with_citations(docs):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n",
    "        if doc.metadata.get('page'):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "\n",
    "# Build the RAG chain with source attribution\n",
    "def generate_attributed_response(question):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format sources with citation numbers\n",
    "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
    "    \n",
    "    # Create the attribution chain using LCEL\n",
    "    attribution_chain = (\n",
    "        attribution_prompt\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Generate the response with citations\n",
    "    response = attribution_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"sources\": sources_formatted\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "question = \"How do transformer models work and what are some examples?\"\n",
    "attributed_answer = generate_attributed_response(question)\n",
    "print(attributed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce595af-3847-4903-bbc5-8c1b19ada5db",
   "metadata": {},
   "source": [
    "# Self-consistency Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e45eede9-bdbb-4625-bfc8-f340fef38ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"claims\": [\n",
      "        {\n",
      "            \"claim\": \"The transformer architecture was introduced by OpenAI in 2018\",\n",
      "            \"status\": \"contradicted\",\n",
      "            \"evidence\": \"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\",\n",
      "            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture was actually introduced in 2017 by Vaswani et al., not by OpenAI in 2018.\"\n",
      "        },\n",
      "        {\n",
      "            \"claim\": \"The transformer architecture uses recurrent neural networks\",\n",
      "            \"status\": \"contradicted\",\n",
      "            \"evidence\": \"It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\",\n",
      "            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture does not use recurrent neural networks, but rather self-attention mechanisms.\"\n",
      "        },\n",
      "        {\n",
      "            \"claim\": \"BERT is a transformer model developed by Google\",\n",
      "            \"status\": \"fully_supported\",\n",
      "            \"evidence\": \"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\",\n",
      "            \"explanation\": \"This claim is fully supported by the provided context.\"\n",
      "        }\n",
      "    ],\n",
      "    \"fully_grounded\": false,\n",
      "    \"issues_identified\": [\"The answer contains incorrect information about the introduction of the transformer architecture and its use of recurrent neural networks.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Dict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def verify_response_accuracy(\n",
    "    retrieved_docs: List[Document],\n",
    "    generated_answer: str,\n",
    "    llm: ChatOpenAI = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Verify if a generated answer is fully supported by the retrieved documents.\n",
    "    Args:\n",
    "        retrieved_docs: List of documents used to generate the answer\n",
    "        generated_answer: The answer produced by the RAG system\n",
    "        llm: Language model to use for verification\n",
    "    Returns:\n",
    "        Dictionary containing verification results and any identified issues\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Define verification prompt - fixed to avoid JSON formatting issues in the template\n",
    "    verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    As a fact-checking assistant, verify whether the following answer is fully supported\n",
    "    by the provided context. Identify any statements that are not supported or contradict the context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer to verify:\n",
    "    {answer}\n",
    "    \n",
    "    Perform a detailed analysis with the following structure:\n",
    "    1. List any factual claims in the answer\n",
    "    2. For each claim, indicate whether it is:\n",
    "       - Fully supported (provide the supporting text from context)\n",
    "       - Partially supported (explain what parts lack support)\n",
    "       - Contradicted (identify the contradiction)\n",
    "       - Not mentioned in context\n",
    "    3. Overall assessment: Is the answer fully grounded in the context?\n",
    "    \n",
    "    Return your analysis in JSON format with the following structure:\n",
    "    {{\n",
    "      \"claims\": [\n",
    "        {{\n",
    "          \"claim\": \"The factual claim\",\n",
    "          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n",
    "          \"evidence\": \"Supporting or contradicting text from context\",\n",
    "          \"explanation\": \"Your explanation\"\n",
    "        }}\n",
    "      ],\n",
    "      \"fully_grounded\": true|false,\n",
    "      \"issues_identified\": [\"List any specific issues\"]\n",
    "    }}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create verification chain using LCEL\n",
    "    verification_chain = (\n",
    "        verification_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Run verification\n",
    "    result = verification_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"answer\": generated_answer\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "retrieved_docs = [\n",
    "    Document(page_content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\"),\n",
    "    Document(page_content=\"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\")\n",
    "]\n",
    "\n",
    "generated_answer = \"The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google.\"\n",
    "\n",
    "verification_result = verify_response_accuracy(retrieved_docs, generated_answer)\n",
    "print(verification_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074f2c8-bdf6-4952-94a2-010cfdb8e4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
