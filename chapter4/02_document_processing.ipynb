{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd232db6-2878-49ee-b94b-7dc1b54d225d",
   "metadata": {},
   "source": [
    "**Make sure you load the API keys for cloud providers!**\n",
    "\n",
    "You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df4aba5-37ba-40a2-99ab-54f31fc08f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables, the keys\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "# for the keys - as explained early in chapter 2\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978e72d-39e1-42d5-8d2b-f1b59fcaab2a",
   "metadata": {},
   "source": [
    "# Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b80c72-cc6e-4a2b-8270-c29cccd28b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'), Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'), Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(metadata={'source': '/home/ben/generative_ai_with_langchain/chapter4/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True\n",
    ")\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b3099-2673-4c78-a84d-066305a9e77b",
   "metadata": {},
   "source": [
    "# Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941153fe-339f-48f9-bbe5-559c56d428e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 chunks from document\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \", # Split on spaces to avoid breaking words\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(chunks)} chunks from document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162a28b-3987-47a9-a08e-9948b271ee87",
   "metadata": {},
   "source": [
    "# Recursive Character Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1674b9-3318-4855-9451-a4de84e0af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.', 'It helps address hallucinations by grounding responses in retrieved information.', '## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4. Augmentation\\n5. Generation', '### Document Processing\\nThis step involves loading and chunking documents appropriately.']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "document = \"\"\"# Introduction to RAG\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.\n",
    "\n",
    "It helps address hallucinations by grounding responses in retrieved information.\n",
    "\n",
    "## Key Components\n",
    "RAG consists of several components:\n",
    "1. Document processing\n",
    "2. Vector embedding\n",
    "3. Retrieval\n",
    "4. Augmentation\n",
    "5. Generation\n",
    "\n",
    "### Document Processing\n",
    "This step involves loading and chunking documents appropriately.\n",
    "\"\"\"\n",
    "\n",
    "chunks = text_splitter.split_text(document)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789f3ec-22d9-45d1-aae9-a4e191702b63",
   "metadata": {},
   "source": [
    "# Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663d02dd-2144-4249-88ec-dae7bd540482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    add_start_index=True  # Include position metadata\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f577ea-c766-408a-a6b5-cdac4595b732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models. It helps address hallucinations by grounding responses in retrieved information. ## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4.',\n",
       " 'Augmentation\\n5. Generation\\n\\n### Document Processing\\nThis step involves loading and chunking documents appropriately. ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34542640-c91b-4d20-9424-551faccadd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
