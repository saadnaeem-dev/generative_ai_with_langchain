{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd1a0dd-25b3-4d61-939c-cef759afced6",
   "metadata": {},
   "source": [
    "**Make sure you load the API keys for cloud providers!**\n",
    "\n",
    "You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e68f8b4-67ce-4fd6-9369-0884b592de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables, the keys\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "# for the keys - as explained early in chapter 2\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c0f3-1603-4748-89d2-b44106600acd",
   "metadata": {},
   "source": [
    "# Exact Match Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d7be71-7ba7-4e81-9d04-b23f01972441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match result (correct answer): {'score': 1}\n",
      "Exact match result (incorrect answer): {'score': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator, ExactMatchStringEvaluator\n",
    "\n",
    "prompt = \"What is the current Federal Reserve interest rate?\"\n",
    "reference_answer = \"0.25%\"  # Suppose this is the correct answer.\n",
    "\n",
    "# Example predictions:\n",
    "prediction_correct = \"0.25%\"\n",
    "prediction_incorrect = \"0.50%\"\n",
    "\n",
    "# Initialize an Exact Match evaluator that ignores case differences.\n",
    "exact_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n",
    "\n",
    "# Evaluate the correct prediction.\n",
    "exact_result_correct = exact_evaluator.evaluate_strings(\n",
    "    prediction=prediction_correct, reference=reference_answer\n",
    ")\n",
    "print(\"Exact match result (correct answer):\", exact_result_correct)\n",
    "\n",
    "# Evaluate an incorrect prediction.\n",
    "exact_result_incorrect = exact_evaluator.evaluate_strings(\n",
    "    prediction=prediction_incorrect, reference=reference_answer\n",
    ")\n",
    "print(\"Exact match result (incorrect answer):\", exact_result_incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de484a-fea6-4bb0-90ac-785aaa35588b",
   "metadata": {},
   "source": [
    "# LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ebb0b48-46b9-409d-b1e5-6352e8ef3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance Evaluation Result:\n",
      "{'reasoning': \"The assistant's response is not verifiable as it does not provide a date or source for the information. The Federal Reserve interest rate changes over time and is not static. Therefore, without a specific date or source, the information provided could be incorrect. The assistant should have advised the user to check the Federal Reserve's official website or a reliable financial news source for the most current rate. The response lacks depth and accuracy. Rating: [[3]]\", 'score': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.evaluation.scoring import ScoreStringEvalChain\n",
    "\n",
    "# Initialize the evaluator LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "# Create the ScoreStringEvalChain from the LLM\n",
    "chain = ScoreStringEvalChain.from_llm(llm=llm)\n",
    "\n",
    "# Define the finance-related input, prediction, and reference answer\n",
    "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
    "finance_prediction = \"The current interest rate is 0.25%.\"\n",
    "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
    "\n",
    "# Evaluate the prediction using the scoring chain\n",
    "result_finance = chain.evaluate_strings(\n",
    "    input=finance_input,\n",
    "    prediction=finance_prediction,\n",
    ")\n",
    "\n",
    "print(\"Finance Evaluation Result:\")\n",
    "print(result_finance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb4d87-aa99-400c-8357-9f528af06e04",
   "metadata": {},
   "source": [
    "# LLM-as-Judge Evaluation with Reference Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae0de7c-9426-439f-91a1-d4cead4e64ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance Evaluation Result (with reference):\n",
      "{'reasoning': \"The assistant's response is helpful, relevant, and correct. It directly answers the user's question about the current Federal Reserve interest rate. However, it lacks depth as it does not provide any additional information or context about the interest rate, such as how it is determined or what it means for the economy. Rating: [[8]]\", 'score': 8}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.evaluation.scoring import LabeledScoreStringEvalChain\n",
    "\n",
    "# Initialize the evaluator LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "# Create the evaluation chain that can use reference answers\n",
    "labeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)\n",
    "\n",
    "# Define the finance-related input, prediction, and reference answer\n",
    "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
    "finance_prediction = \"The current interest rate is 0.25%.\"\n",
    "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
    "\n",
    "# Evaluate the prediction against the reference\n",
    "labeled_result = labeled_chain.evaluate_strings(\n",
    "    input=finance_input,\n",
    "    prediction=finance_prediction,\n",
    "    reference=finance_reference,\n",
    ")\n",
    "\n",
    "print(\"Finance Evaluation Result (with reference):\")\n",
    "print(labeled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658969d-1e31-45c3-99a2-4a7127a1be0c",
   "metadata": {},
   "source": [
    "# Criteria Evaluation (Tone and Conciseness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d00b3a-ace4-481e-85a2-391d4d6fe3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conciseness evaluation result: {'reasoning': \"The criterion is conciseness. This means the submission should be brief, to the point, and not contain unnecessary information.\\n\\nLooking at the submission, it provides a direct answer to the question, stating that a normal blood pressure reading is around 120/80 mmHg. This is a concise answer to the question.\\n\\nThe submission also includes an additional sentence advising to follow a doctor's advice for personal health management. While this information is not directly related to the question, it is still relevant and does not detract from the conciseness of the answer.\\n\\nTherefore, the submission meets the criterion of conciseness.\\n\\nY\", 'value': 'Y', 'score': 1}\n",
      "Friendliness evaluation result: {'reasoning': \"The criterion is to assess whether the response is written in a friendly and approachable tone. The submission provides the information in a straightforward manner and ends with a suggestion to follow doctor's advice for personal health management. This suggestion can be seen as a friendly advice, showing concern for the reader's health. Therefore, the submission can be considered as written in a friendly and approachable tone.\\n\\nY\", 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "evaluation_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "prompt_health = \"What is a healthy blood pressure range for adults?\"\n",
    "prediction_health = (\n",
    "    \"A normal blood pressure reading is typically around 120/80 mmHg. \"\n",
    "    \"It's important to follow your doctor's advice for personal health management!\"\n",
    ")\n",
    "\n",
    "# Evaluate conciseness\n",
    "conciseness_evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "conciseness_result = conciseness_evaluator.evaluate_strings(\n",
    "    prediction=prediction_health, input=prompt_health\n",
    ")\n",
    "print(\"Conciseness evaluation result:\", conciseness_result)\n",
    "\n",
    "# Evaluate friendliness with custom criterion\n",
    "custom_friendliness = {\n",
    "    \"friendliness\": \"Is the response written in a friendly and approachable tone?\"\n",
    "}\n",
    "friendliness_evaluator = load_evaluator(\"criteria\", criteria=custom_friendliness, llm=evaluation_llm)\n",
    "friendliness_result = friendliness_evaluator.evaluate_strings(\n",
    "    prediction=prediction_health, input=prompt_health\n",
    ")\n",
    "print(\"Friendliness evaluation result:\", friendliness_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e40b9d-f51d-4c0a-8b63-f87efe960c91",
   "metadata": {},
   "source": [
    "# JSON Format Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab0e38c-b038-4b90-8624-6ffc05f1e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON validity result (valid): {'score': 1}\n",
      "JSON validity result (invalid): {'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 63 (char 62)'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonValidityEvaluator\n",
    "\n",
    "# Initialize the JSON validity evaluator.\n",
    "json_validator = JsonValidityEvaluator()\n",
    "\n",
    "valid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000}'\n",
    "invalid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000,}'\n",
    "\n",
    "# Evaluate the valid JSON.\n",
    "valid_result = json_validator.evaluate_strings(prediction=valid_json_output)\n",
    "print(\"JSON validity result (valid):\", valid_result)\n",
    "\n",
    "# Evaluate the invalid JSON.\n",
    "invalid_result = json_validator.evaluate_strings(prediction=invalid_json_output)\n",
    "print(\"JSON validity result (invalid):\", invalid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597380d7-21f6-44b5-a73f-471ae9c8151c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
