{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9526d8b-e253-4287-a2c4-dcd44af68b4e",
   "metadata": {},
   "source": [
    "**Make sure you load the API keys for cloud providers!**\n",
    "\n",
    "You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe4f764-e4d4-4ff3-ba6a-11fcbdfc2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables, the keys\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "# for the keys - as explained early in chapter 2\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff5b83b3-cfcf-4b4f-ab20-e7e0c3b33c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"***REMOVED***\"\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "582c752e-ad74-4adb-bed9-905e42c8d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: Hello! How can I assist you today?\n",
      "\n",
      "This run has been logged to LangSmith.\n",
      "You can view it in the LangSmith UI: https://smith.langchain.com\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a simple LLM call that will be traced in LangSmith\n",
    "llm = ChatOpenAI()\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(f\"Model response: {response.content}\")\n",
    "print(\"\\nThis run has been logged to LangSmith.\")\n",
    "print(\"You can view it in the LangSmith UI: https://smith.langchain.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2bc30-3e87-469e-8245-3ab1d5170211",
   "metadata": {},
   "source": [
    "# Creating an evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5b6171b-4dab-478e-afb1-5dc29b61208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 2 examples\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "# Sample financial examples\n",
    "financial_examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What are the tax implications of early 401(k) withdrawal?\",\n",
    "            \"context_needed\": [\"retirement\", \"taxation\", \"penalties\"]\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"Early withdrawals from a 401(k) typically incur a 10% penalty if you're under 59Â½ years old, in addition to regular income taxes. However, certain hardship withdrawals may qualify for penalty exemptions.\",\n",
    "            \"key_points\": [\"10% penalty\", \"income tax\", \"hardship exemptions\"],\n",
    "            \"documents\": [\"IRS publication 575\", \"Retirement plan guidelines\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"How does dollar-cost averaging compare to lump-sum investing?\",\n",
    "            \"context_needed\": [\"investment strategy\", \"risk management\", \"market timing\"]\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"Dollar-cost averaging spreads investments over time to reduce timing risk, while lump-sum investing typically outperforms in rising markets due to longer market exposure. DCA may provide psychological benefits through reduced volatility exposure.\",\n",
    "            \"key_points\": [\"timing risk\", \"market exposure\", \"psychological benefits\"],\n",
    "            \"documents\": [\"Investment strategy comparisons\", \"Market timing research\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create dataset in LangSmith\n",
    "dataset_name = \"Financial Advisory RAG Evaluation\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Evaluation dataset for financial advisory RAG systems covering retirement, investments, and tax planning.\"\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "for example in financial_examples:\n",
    "    client.create_example(\n",
    "        inputs=example[\"inputs\"],\n",
    "        outputs=example[\"outputs\"],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "print(f\"Created evaluation dataset with {len(financial_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ab707-bf6b-488c-954d-8913b41e4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation configuration\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "# Define evaluation criteria specific to RAG systems\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # Correctness: Compare response to reference answer\n",
    "        RunEvalConfig.LLM(\n",
    "            criteria={\n",
    "                \"factual_accuracy\": \"Does the response contain only factually correct information consistent with the reference answer?\"\n",
    "            }\n",
    "        ),\n",
    "        # Groundedness: Ensure response is supported by retrieved context\n",
    "        RunEvalConfig.LLM(\n",
    "            criteria={\n",
    "                \"groundedness\": \"Is the response fully supported by the retrieved documents without introducing unsupported information?\"\n",
    "            }\n",
    "        ),\n",
    "        # Retrieval quality: Assess relevance of retrieved documents\n",
    "        RunEvalConfig.LLM(\n",
    "            criteria={\n",
    "                \"retrieval_relevance\": \"Are the retrieved documents relevant to answering the question?\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed3fa3-0ff6-4267-a577-761b8460c655",
   "metadata": {},
   "source": [
    "## Function to construct your RAG chain (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4ae4a-1733-4281-a183-25be24a219a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_chain():\n",
    "    # This would be your actual RAG implementation\n",
    "    # For example: return RAGChain(...)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf1fb5-1116-4fb0-9eec-a41f0132f503",
   "metadata": {},
   "source": [
    "## Run evaluation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98f815-f7bf-48c7-9bc3-68af991968ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import run_on_dataset\n",
    "results = run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=construct_chain,\n",
    "    evaluation=evaluation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0dbdb-b718-4e74-870e-d3fb5cbe5665",
   "metadata": {},
   "source": [
    "# Insurance Claim Extraction Evaluation Example"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a191fba8-4378-40b9-9b0f-51aeb8936602",
   "metadata": {},
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Define a list of synthetic insurance claim examples\n",
    "example_inputs = [\n",
    "    (\n",
    "        \"I was involved in a car accident on 2023-08-15. My name is Jane Smith, Claim ID INS78910, \"\n",
    "        \"Policy Number POL12345, and the damage is estimated at $3500.\",\n",
    "        {\n",
    "            \"claimant_name\": \"Jane Smith\",\n",
    "            \"claim_id\": \"INS78910\",\n",
    "            \"policy_number\": \"POL12345\",\n",
    "            \"claim_amount\": \"$3500\",\n",
    "            \"accident_date\": \"2023-08-15\",\n",
    "            \"accident_description\": \"Car accident causing damage\",\n",
    "            \"status\": \"pending\"\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        \"My motorcycle was hit in a minor collision on 2023-07-20. I am John Doe, with Claim ID INS112233 \"\n",
    "        \"and Policy Number POL99887. The estimated damage is $1500.\",\n",
    "        {\n",
    "            \"claimant_name\": \"John Doe\",\n",
    "            \"claim_id\": \"INS112233\",\n",
    "            \"policy_number\": \"POL99887\",\n",
    "            \"claim_amount\": \"$1500\",\n",
    "            \"accident_date\": \"2023-07-20\",\n",
    "            \"accident_description\": \"Minor motorcycle collision\",\n",
    "            \"status\": \"pending\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Insurance Claims\"\n",
    "\n",
    "# Create the dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Synthetic dataset for insurance claim extraction tasks\",\n",
    ")\n",
    "\n",
    "# Store examples in the dataset\n",
    "for input_text, expected_output in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"input\": input_text},\n",
    "        outputs={\"output\": expected_output},\n",
    "        metadata={\"source\": \"Synthetic\"},\n",
    "        dataset_id=dataset.id,\n",
    "    )\n",
    "\n",
    "# Define the extraction schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class InsuranceClaim(BaseModel):\n",
    "    claimant_name: str = Field(..., description=\"The name of the claimant\")\n",
    "    claim_id: str = Field(..., description=\"The unique insurance claim identifier\")\n",
    "    policy_number: str = Field(..., description=\"The policy number associated with the claim\")\n",
    "    claim_amount: str = Field(..., description=\"The claimed amount (e.g., '$5000')\")\n",
    "    accident_date: str = Field(..., description=\"The date of the accident (YYYY-MM-DD)\")\n",
    "    accident_description: str = Field(..., description=\"A brief description of the accident\")\n",
    "    status: str = Field(\"pending\", description=\"The current status of the claim\")\n",
    "\n",
    "# Create extraction chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "instructions = (\n",
    "    \"Extract the following structured information from the insurance claim text: \"\n",
    "    \"claimant_name, claim_id, policy_number, claim_amount, accident_date, \"\n",
    "    \"accident_description, and status. Return the result as a JSON object following \"\n",
    "    \"this schema: \" + InsuranceClaim.schema_json()\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0).bind_functions(\n",
    "    functions=[InsuranceClaim.schema()],\n",
    "    function_call=\"InsuranceClaim\"\n",
    ")\n",
    "\n",
    "output_parser = JsonOutputFunctionsParser()\n",
    "extraction_chain = instructions | llm | output_parser | (lambda x: {\"output\": x})\n",
    "\n",
    "# Test the extraction chain\n",
    "sample_claim_text = (\n",
    "    \"I was involved in a car accident on 2023-08-15. My name is Jane Smith, \"\n",
    "    \"Claim ID INS78910, Policy Number POL12345, and the damage is estimated at $3500. \"\n",
    "    \"Please process my claim.\"\n",
    ")\n",
    "\n",
    "result = extraction_chain.invoke({\"input\": sample_claim_text})\n",
    "print(\"Extraction Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aabb7e-22c8-4a28-8011-1cdaff45ca40",
   "metadata": {},
   "source": [
    "# HuggingFace evaluation for code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db75ecc-4cf3-40a8-8b70-2aa49ba3b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8db5dac-0e68-44d2-8e30-7b80094b98e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass@1': 0.5, 'pass@2': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "human_eval = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "code_eval_metric = load(\"code_eval\")\n",
    "\n",
    "test_cases = [\"assert add(2,3)==5\"]\n",
    "candidates = [[\"def add(a,b): return a*b\", \"def add(a, b): return a+b\"]]\n",
    "\n",
    "pass_at_k, results = code_eval_metric.compute(references=test_cases, predictions=candidates, k=[1, 2])\n",
    "print(pass_at_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029395c-5759-4e1b-bc13-1037eee5a3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
