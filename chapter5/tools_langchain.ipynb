{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XoS_YXhsRGQD",
      "metadata": {
        "id": "XoS_YXhsRGQD"
      },
      "source": [
        "# Tools on LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sNjI6aGKBzAJ0aIcO0YDYpWs",
      "metadata": {
        "id": "sNjI6aGKBzAJ0aIcO0YDYpWs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "llm = ChatVertexAI(model=\"gemini-2.0-flash-001\")\n",
        "question = \"how old is the US president?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d163ea1d",
      "metadata": {},
      "source": [
        "Modern LLMs hide the need to construct a prompt from the user, you can define your tools as schemas instead and pass them as a separate argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JzrgWDIKy3Y2",
      "metadata": {
        "id": "JzrgWDIKy3Y2"
      },
      "outputs": [],
      "source": [
        "search_tool = {\n",
        "    \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
        "    \"title\": \"google_search\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"query\": {\n",
        "            \"description\": \"Search query to be sent to the search engine\",\n",
        "            \"title\": \"search_query\",\n",
        "            \"type\": \"string\"},\n",
        "    },\n",
        "    \"required\": [\"query\"]\n",
        "}\n",
        "step1 = llm.invoke(question, tools=[search_tool])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24518efd",
      "metadata": {},
      "source": [
        "As we can see, now our outputs contains a special part called `tool_calls` - again, there's no need for us to parse the output any more, it's all done by an LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-pvtbxJDSLvv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1738142152475,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "-pvtbxJDSLvv",
        "outputId": "8c7d65df-c333-4faa-9ee0-6063d607701d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'google_search', 'args': {'query': 'age of Donald Trump'}, 'id': 'd4dc04b5-a554-410a-9734-765e44cefa91', 'type': 'tool_call'}]\n"
          ]
        }
      ],
      "source": [
        "print(step1.tool_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c4645a",
      "metadata": {},
      "source": [
        "We can pass the tool calling result back to an LLM as a special `ToolMessage` message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3J3M4s3SQdm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 927,
          "status": "ok",
          "timestamp": 1738142246299,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "c3J3M4s3SQdm",
        "outputId": "ae26ab65-b059-41fb-a091-7f23101fbd2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Donald Trump is 78 years old.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "\n",
        "tool_result = ToolMessage(content=\"Donald Trump â€º Age 78 years June 14, 1946\\n\", tool_call_id=step1.tool_calls[0][\"id\"])\n",
        "step2 = llm.invoke([\n",
        "    HumanMessage(content=question), step1, tool_result], tools=[search_tool])\n",
        "assert len(step2.tool_calls) == 0\n",
        "\n",
        "print(step2.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c54c82b",
      "metadata": {},
      "source": [
        "For the convinience, we can also `bind` tools to an LLM so that they would be auto-added to arguments on every invocation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4uThVOXvVfQk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2038,
          "status": "ok",
          "timestamp": 1738142853415,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "4uThVOXvVfQk",
        "outputId": "5161dfd1-8d02-43ea-b433-1a189dfc4df9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'google_search', 'arguments': '{\"query\": \"age of US president Joe Biden\"}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 36, 'candidates_token_count': 10, 'total_token_count': 46, 'cached_content_token_count': 0}, 'finish_reason': 'STOP', 'avg_logprobs': -0.07167894244194031}, id='run-519331b6-82a5-4f9b-9102-2b123544346b-0', tool_calls=[{'name': 'google_search', 'args': {'query': 'age of US president Joe Biden'}, 'id': '12dbb680-e7fe-4c06-9f54-cfcb68070c61', 'type': 'tool_call'}], usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46})"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_with_tools = llm.bind(tools=[search_tool])\n",
        "llm_with_tools.invoke(question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Chapter2.5",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
