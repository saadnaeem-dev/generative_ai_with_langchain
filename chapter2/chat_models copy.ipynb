{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0b91b3-2bb2-49ee-b43e-eb53eabc2b50",
   "metadata": {},
   "source": [
    "# Basic examples\n",
    "\n",
    "**Make sure you load the API keys for cloud providers!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd807e9-036c-44de-8752-1ef5fb23531f",
   "metadata": {},
   "source": [
    "**You can set your environment keys yourself or use a script. Please note that since keys are private, they are not included in the repository.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294273f9-3852-49a4-86b1-9522e940725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables, the keys\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "# for the keys - as explained early in chapter 2\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d0a8a-c0a0-4e4a-9dc3-36cc1de83a43",
   "metadata": {},
   "source": [
    "You can use the same interface to work with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c523bfd-c9b7-426a-b02c-58b51039ca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why couldn't the bicycle stand up by itself? Because it was two-tired!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# initialize Gemini\n",
    "gemini_pro = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "# or: initialize OpenAI model\n",
    "openai_llm = OpenAI()\n",
    "\n",
    "# Both can be used with the same interface\n",
    "# response = openai_llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "response = openai_llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ee5aa-fbf5-4a3e-ba62-2ef216174527",
   "metadata": {},
   "source": [
    "For testing, there's the FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8af08df-bc6f-4769-9591-6b51ea586cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables (if needed for other parts of your project)\n",
    "load_dotenv()\n",
    "\n",
    "# Create a fake LLM that always returns the same responses\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "\n",
    "# Test the fake LLM\n",
    "result = fake_llm.invoke(\"Any input will return Hello\")\n",
    "print(result)  # Output: Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f341d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from langchain_community.llms.fake import FakeListLLM\n",
    "\n",
    "# Create a fake LLM that always returns the same responses\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "\n",
    "result = fake_llm.invoke(\"Any input will return Hello\")\n",
    "print(result)  # Output: Hello\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836403-b8ca-4f64-8f87-482b27bfead4",
   "metadata": {},
   "source": [
    "The default interface to work with LLMs if the Chat interface though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "# or: from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# specifying a model:\n",
    "chat = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "# or:\n",
    "# chat = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "# messages:\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful programming assistant\"),\n",
    "    HumanMessage(content=\"Write a Python function to calculate factorial\")\n",
    "]\n",
    "response = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8e1e29-b19a-47d7-9b54-cc8a2a6f1d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a Python function that calculates the factorial of a non-negative integer:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    \n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "1. The function takes a single parameter `n`, which represents the number for which we want to calculate the factorial.\n",
      "\n",
      "2. We start by checking if `n` is negative. If it is, we raise a `ValueError` because factorial is not defined for negative numbers.\n",
      "\n",
      "3. Next, we check if `n` is equal to 0 or 1. If it is, we return 1 since the factorial of 0 and 1 is defined as 1.\n",
      "\n",
      "4. If `n` is greater than 1, we initialize a variable `result` to 1. This variable will store the factorial value.\n",
      "\n",
      "5. We use a `for` loop to iterate from 2 to `n` (inclusive). In each iteration, we multiply `result` by the current value of `i`. This effectively calculates the factorial by multiplying all the numbers from 2 to `n`.\n",
      "\n",
      "6. Finally, we return the calculated factorial value stored in `result`.\n",
      "\n",
      "Here are a few examples of how to use the `factorial` function:\n",
      "\n",
      "```python\n",
      "print(factorial(0))  # Output: 1\n",
      "print(factorial(1))  # Output: 1\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(10))  # Output: 3628800\n",
      "print(factorial(-5))  # Raises ValueError: Factorial is not defined for negative numbers.\n",
      "```\n",
      "\n",
      "The time complexity of this factorial function is O(n) since it iterates from 2 to n. The space complexity is O(1) as it only uses a constant amount of additional space.\n",
      "\n",
      "Note: For larger values of `n`, the factorial can quickly become very large and may exceed the maximum integer value that can be represented in Python. In such cases, you may need to use a library like `math.factorial()` or implement a more advanced algorithm to handle large factorial calculations.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c814c7-c755-413e-8798-2960ab8bd28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
